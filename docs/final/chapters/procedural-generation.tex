\chapter{Procedural Maze Generation} \label{procedural}
\begin{figure}[!h]
\centering
    \includegraphics[width=.4\textwidth]{images/maze_example.png}
    \caption{Example maze generated by our program. The start cell is yellow and the goalstate is red. Base CA: B1/S2345, Bias $\lambda$ = 0.6}
\label{fig:maze-example}
\end{figure}
We build an evolutionary algorithm toolkit to evolve life-like CA for procedural generation. Maze generation is picked as an appropriately challenging domain since the quality of a maze is dictated by global properties. Figure~\ref{fig:maze-example} is an example of a maze generated by our program which we use throughout this chapter. These properties cannot be directly encoded in the chromosome of life-like CA. To generate a good maze, a CA rule must induce the birth and survival of cells at times that increase the likelihood of corridor and junction-like structures forming. Mazes possess many structural similarities to life-like CA including a lattice shape and two mutually-exclusive states, \textit{wall} and \textit{path}. Furthermore, two mazes with similar global properties tend to be phenotypically alike which simplifies evolutionary learning. However, CA are stochastic systems and provide no guarantees that any maze generated will be solvable. To mitigate this, we implement a repair mechanism that amends near-valid solutions and discards solutions that do not meet a minimum criterion. This allows genes that contribute to high quality solutions to survive while maintaining a level of selection pressure on solution validity alongside solution quality.\\

The task of CA-based maze generation has some similarities to a work by C. Adams\cite{adams2018evolving}. However, this application differs notably from Adams' in both the evolution algorithm and fitness function design. Key features in our maze generator include the notion of failed rules, the stochastic region merging algorithm, and automated loss calculation (i.e. no external human input required to rank mazes).\\

The EA toolkit is written entirely in Python and features the following key classes:
\begin{itemize}
    \item Experiment Suite: Includes methods and configurations for running evolutionary experiments.
    \item Population: Handles population-wide actions like crossover, elitism, and selection. It also tracks metrics for hyperparameter tuning.
    \item Chromosome: Handles individual actions like mutation, and fitness calculation.
    \item Simulator: Runs binary-state CA, performs repair, and caches state data when needed.
\end{itemize}
Each time we learn a new class of CA or optimise for a new objective, we write a new instance of the Population,  Chromosome, and Simulator classes.

\section{Simulator} \label{subsec:simulator-1}
Due to the undecidability of various CA rules, the state of a CA after some time cannot, in general, be calculated without simulating each transition in turn. For this, a CA simulator was built. Figure~\ref{fig:stains} shows snapshots from an example simulation.

\begin{figure}[!h]
\centering
            \subfloat[$t=0$]{\includegraphics[width=.2\textwidth]{images/stains/0.png}}\hfill
            \subfloat[$t=1$]{\includegraphics[width=.2\textwidth]{images/stains/1.png}}\hfill
            \subfloat[$t=2$]{\includegraphics[width=.2\textwidth]{images/stains/2.png}}\hfill
            \subfloat[$t=3$]{\includegraphics[width=.2\textwidth]{images/stains/3.png}}\hfill
            \subfloat[$t=4$]{\includegraphics[width=.2\textwidth]{images/stains/4.png}}\hfill
            \caption{5 consecutive snapshots of \textit{Stains}, a stable rule with rulestring B3678/S235678, created in the simulator}
\label{fig:stains}
\end{figure}

The simulator stores the state of a 2D square CA of side length $N$ in an $N \times N$ matrix. The CA is initialised with birth set $B$ and survival set $S$ which are given as direct arguments or calculated from the chromosome. When simulating $n$ time steps, the simulator begins by caching the current state $X^{(t)}$. Then a neighbourhood matrix $M$ is calculated by convolving $X^{(t)}$ with kernel $\kappa$ where
\begin{equation}
    \kappa = \begin{bmatrix}
        1 & 1 & 1\\
        1 & 0 & 1\\
        1 & 1 & 1
        \end{bmatrix}    
\end{equation}
The value $M_{i,j}$ is the number of live neighbours of $X^{(t)}_{i,j}$. The convolution is calculated with wrapped boundaries to simulate periodic boundary conditions. The next state is calculated as follows
\begin{equation}
    X^{(t+1)}_{i,j}= (\lnot X^{(t)}_{i,j} \land n \in B) \lor (X^{(t)}_{i,j} \land n \in S)
\end{equation}
where the left conjunction corresponds to the case of a dead cell becoming alive and the right conjunction corresponds to a living cell surviving. If $X^{(t+1)} = X^{(t)}$, where $X^{(t)}$ is the cached state, then no further steps are calculated since $X^{(t + n)} = X^{(t + n - 1)} = ... = X^{(t)}$ by induction. Otherwise, the current state is cached and the simulator continues until $n$ steps have elapsed or a later fixed point is reached. The simulator does not automatically detect periods of length greater than one step as this would involve maintaining and comparing against a large history of previous states which would be too computationally intensive. The simulator allows the initial state $X^{(0)}$ to be set randomly with a particular density or to be set explicitly.\\

\section{Procedural Generation} \label{sec:proc-gen}
A maze is generated from a chromosome in three stages: growth, region search, and region merging. During the growth stage, a CA is run in the simulator for a fixed number of iterations, typically 50, using the birth and survival sets encoded in the chromosome. 

\subsection{Region Search}

The region search stage, defined by Algorithm~\ref{alg:region-find}, uses randomly initialised breadth-first searches to find all disconnected regions within the maze. In order to perform the merge stage later on, two data structures are populated during the search stage. The first is a hashmap from each cell to the index of the region it occupies. The second is the reverse, a hashmap from each index to the set of cells in that region.

\begin{algorithm}
  \caption{Region Search Algorithm}\label{alg:region-find}
  \begin{algorithmic}
  \Require $X$ - the state of the CA after the growth stage
  \Ensure cells[($c_x$, $c_y$)] = $r_c \iff$ ($c_x$, $c_y$) $\in$ regions[$r_c$]

  \Comment{Initialisation}
  \State cells $\gets$ empty dictionary of type \{(int, int): int\}
  \State regions $\gets$ empty dictionary of type \{int: set\{int\}\}
  \State spaces $\gets$ set of cells in X with state 0

  \Comment{Find first region}
  \State $r_1 \gets$ \Call{BFS}{start-cell, $X$}
  \State \Call{UpdateDicts}{$r_1$, 1}
  
  \Comment{Find remaining regions}
  \State counter $\gets 1$
  \While{spaces not empty}
    \State counter $\gets$ counter + 1
    \State startCell $\gets$ randomly chosen 0-state cell
    \State $r \gets$ \Call{BFS}{startCell, $X$}
    \State \Call{UpdateDicts}{$r$, counter}
  \EndWhile

  \Comment{Update Function}
  \Procedure{UpdateDicts}{region, index}
    \For{$c$ in region}
        \State cells[$c$] = index
        \State regions[index].add($c$)
    \EndFor
    \State spaces $\gets$ spaces - $r$
  \EndProcedure
  \end{algorithmic}
\end{algorithm}

Figure~\ref{fig:region-search} shows the region search algorithm.

\begin{figure}[H]
\centering
            \hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_search/1.png}}\hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_search/2.png}}\hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_search/3.png}}\hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_search/4.png}}\hfill
            \hfill
            \caption{Snapshots from region search algorithm. Yellow represents visited path cells and red represents the goal cell. Time moves from left to right.}
\label{fig:region-search}
\end{figure}

\subsection{Region Merge}

The region merge stage, defined by Algorithm~\ref{alg:region-merge}, connects regions randomly until a connected path exists from the start cell to the goal cell or the maze is deemed invalid. It is crucial for the region merging algorithm to be stochastic. If it is deterministic and merges regions according to a pre-designed pattern, the genetic algorithm is incentivised to learn rules that lend themselves well to this pattern. For example, if mazes with longer solution paths are considered fitter and the merging algorithm connects regions in horizontal bands sweeping left-to-right (as in \cite{adams2018evolving}) then the evolutionary process is incentivised to produce rules with shorter horizontal corridors over longer vertical corridors. This is in direct conflict with the fitness function. To avoid this, we design a stochastic region merging algorithm. It begins with the region containing the start cell. Each wall cell bordering this region is examined to determine whether removing the cell would connect to a distinct region. One of these wall cells is randomly chosen and removed. This process repeats on the union of the two joined regions. If no such wall cells exist, the simulation is deemed invalid. This can be thought of as only permitting walls of width 1 to be broken when connecting regions. Allowing walls of arbitrary thickness to be joined is undesirable as it diminishes the contribution of the CA over the final solution. Consider the extreme case of a maze with only wall cells except the start and end goal. When the merging algorithm connects the start and end, the solution's form is a consequence of the merging algorithm alone and not the CA which generated a maze of walls in the first place. This would clearly be undesirable. If a chromosome does not yield valid simulations at least 80\% of the time, it is assigned a fitness of 0 and usually removed from the population in the following iteration.

\begin{algorithm}
  \caption{Region Merge Algorithm}\label{alg:region-merge}
  \begin{algorithmic}
  \Require cells, regions, X
  \State visited $\gets$ regions[1]
  \While{True}
    \State fringe $\gets$ \Call{OneNeighbours}{visited}
    \If{goalCell in fringe}
        \State return True \Comment{Success}
    \EndIf
    \State candidates $\gets$ []
    \For{$f$ in fringe}
        \State zeros $\gets$ \Call{ZeroNeighbours}{$f$}
        \If{length(zeros - visited) > 0}
            \State candidates.append($f$)
        \EndIf
    \EndFor
    \If{length(candidates) > 0}
        \State $c \gets$ \Call{PopRandom}{candidates}
        \State visited.add($c$)
        \State X[$c$] = 0
        \State newRegions $\gets$ \{cells[$d$] for $d \in$ \Call{ZeroNeighbours}{$c$}\}
        \State visited $\gets$ visited $\cup$ \{regions[$r$] for $r \in$ newRegions\}
    \Else
        \State return False \Comment{Failure}
    \EndIf
  \EndWhile
  \State
  \State where \Call{OneNeighbours}{} and \Call{ZeroNeighbours}{} return the 1-state and 0-state neighbours of a cell respectively.
  \end{algorithmic}
\end{algorithm}

Figure~\ref{fig:region-merge} shows the region merge algorithm.

\begin{figure}[H]
\centering
            \hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_merge/1.png}}\hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_merge/2.png}}\hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_merge/3.png}}\hfill
            \subfloat{\includegraphics[width=.2\textwidth]{images/region_merge/4.png}}\hfill
            \hfill
            \caption{Snapshots from region merge algorithm. Yellow represents visited path cells, red represents the goal cell and green represents fringe wall cells that can be considered for destruction if they border the current region and an unvisited region. Time moves from left to right.}
\label{fig:region-merge}
\end{figure}


\section{Genetic Algorithm I}\label{sec:ga-1}

A genetic algorithm is used to evolve a population of chromosomes that represent the transition rule of the life-like CA. We explore the chromosome representation, genetic operators, and fitness function in this section.

\subsection{Chromosome}
A transition rule can be represented in a number of ways. Most intuitively, consider a birth and survival set indicating the number of neighbours that elicit a dead cell to become alive or a living cell to remain alive respectively. This can be encoded as a binary string or in integer form as shown in Figure~\ref{fig:bs-calc}.\\

\begin{figure}[!h] 
\begin{equation*} 
\begin{split}
    \text{Number of neighbours:}&\ 0\ 1\ 2\ 3\ 4\ 5\ 6\ 7\ 8\ |\ 0\ 1\ 2\ 3\ 4\ 5\ 6\ 7\ 8\\
    \text{Binary representation:}&\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ |\ 0\ 0\ 1\ 1\ 0\ 0\ 0\ 0\ 0\\
    &\qquad\ \: \uparrow \qquad \qquad \qquad \ \: \: \uparrow \ \uparrow\\
    \text{Set representation:}&\quad \ \text{B:} \{3\} \qquad \qquad \ \ \: \text{S:}\{2, 3\}\\
    \text{Integer representation:}&\ \texttt{0b000100000001100000} = 16480
\end{split}   
\end{equation*}
\caption{Different representations of a life-like CA chromosome}
\label{fig:bs-calc}
\end{figure}

Each binary string chromosome has length 18, so the discrete search space is of size $2^{18} = 262144$. An initial population of $\mu$ chromosomes is chosen randomly from a distribution that is uniform over the density of the binary representations. This is preferable to a distribution that is uniform over the integer representations\cite{mitchell1994evolving}. When initialising a random chromosome, a density $\rho$ is picked uniformly from $[0, 1]$. Then, each bit is a sample from the $\mathit{Bernoulli}(\rho)$ distribution.

\subsection{Genetic Operators}
At each iteration, the algorithm explores the search space through a collection of genetic operators. Crossover and mutation produce novel candidates, fitness functions evaluate each individual, and selection concentrates learned information in a set of elite parents. We produce $\lambda$ new children in each expansion stage and reduce down to $\mu$ elite candidates in each contraction stage with $\lambda \approx 4\mu$ as advised by Hansen et al.\cite{hansen2015evolution}. Single-point crossover is used to produce new children. Pointwise mutation is applied with probability $\frac{1}{18}$ such that the expected number of bit flips per chromosome is 1. For efficiency, this is implemented by generating a mutation mask with expected density $\frac{1}{18}$ and applying XOR between the chromosome and mutation mask.\\

% When producing the next generation, we consider selection criteria based on fitness and age. In a $(\mu + \lambda)$ selection setting, we produce a population of $\lambda$ children from $\mu$ parents and the next generation is selected from the collective. In a $(\mu, \lambda)$ setting, the next generation is selected exclusively from the $\lambda$ children. We can interpret this as an age restriction of 1 generation on the $\mu$ parent candidates.

Common forms of selection include roulette and truncation. In truncation selection, all candidates are linearly ranked by fitness and the top $\mu$ candidates progress to the next generation. In roulette selection, the probability of each individual progressing to the next generation is proportional to its objective fitness. Both are implemented and compared in Chapter \ref{evaluation}.

\subsection{Fitness Function}
The aim of the fitness function is to assess the maze using quantitative metrics that can be calculated efficiently. For example, the number of vacant cells reachable from the start cell is an important metric because, if this is too low, a large portion of the maze is wasted space. This metric is computed in linear time with respect to the number of cells in the CA by performing a breadth-first search in the region containing the start cell and recording the number of vacant cells in the maze that exist outside this region. We opt for two metrics: the number of dead ends and the solution path length. A cell is considered to be a dead end if all its neighbours are wall cells or vacant cells that have already been visited. These two factors work well as they are easy to calculate, well-correlated with perceived maze difficulty\cite{adams2018evolving} and, crucially, oppose each other. A maze with a long solution tends to have long corridors, whereas a maze with many dead ends tends to have shorter corridors and more decisions to make at each junction. By optimising on conflicting objectives, we automatically regulate learning speed and reduce the probability of premature convergence to local optima. Assuming a maze with at least one solution, both metrics can be calculated simultaneously in a single breadth-first search traversal. Figure~\ref{fig:dead-ends} shows this process.\\

\begin{figure}[!h]
\centering
            \subfloat{\includegraphics[width=.25\textwidth]{images/dead_ends/1.png}}\hfill
            \subfloat{\includegraphics[width=.25\textwidth]{images/dead_ends/2.png}}\hfill
            \subfloat{\includegraphics[width=.25\textwidth]{images/dead_ends/3.png}}\hfill
            \subfloat{\includegraphics[width=.25\textwidth]{images/dead_ends/4.png}}\hfill
            \caption{Snapshots from BFS algorithm calculating number of dead ends. The green cells represent dead ends discovered. Time moves left to right.}
\label{fig:dead-ends}
\end{figure}

Initially, the fitness function $f(c_i) = s + \lambda d$, where s is the solution path length and d is the number of dead ends, was considered. However, this is not normalised as the solution length and number of dead ends are not on the same scale. Moreover, we cannot normalise each metric individually based on the range of values present in a given generation since the ranges vary from experiment to experiment and generation to generation. Instead, a truncated linear selection is performed with fitness function $f(c_i) = r_s + \lambda r_d$ where $r_s$ and $r_d$ are the rank of the cell in the population according to solution length and number of dead ends respectively.